model,size ,x,y,f1_score,accuracy,precision,recall,训练参数,测试参数
Gemma,2,12,0.571428571,47.5,,76.92307692,0.571428571,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path google/gemma-2b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 16 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Gemma-2B/lora/train_2024-07-07-08-51-0012 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path google/gemma-2b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Gemma-2B/lora/eval_2024-07-02-14-51-38 \
    --do_predict True \
    --adapter_name_or_path saves/Gemma-2B/lora/train_2024-07-02-14-51-38"
Gemma,2,8,0.571428571,47.5,,76.92307692,0.571428571,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path google/gemma-2b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 16 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Gemma-2B/lora/train_2024-07-07-08-51-008 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path google/gemma-2b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Gemma-2B/lora/eval_2024-07-07-08-51-24 \
    --do_predict True \
    --adapter_name_or_path saves/Gemma-2B/lora/train_2024-07-07-08-51-008"
Gemma,2,4,0.571428571,47.5,,76.92307692,0.571428571,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path google/gemma-7b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 16 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Gemma-7B/lora/train_2024-07-02-14-51-40 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path google/gemma-2b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Gemma-2B/lora/eval_2024-07-07-08-51-24 \
    --do_predict True \
    --adapter_name_or_path saves/Gemma-2B/lora/train_2024-07-07-08-51-004"
Gemma,7,4,0.642857143,62.56684492,,67.5,64.28571429,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path google/gemma-7b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Gemma-7B/lora/train_2024-07-07-09-36-20 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --adapter_name_or_path saves/Gemma-7B/lora/train_2024-07-07-09-36-20 \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path google/gemma-7b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Gemma-7B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Gemma-7B/lora/train_2024-07-07-09-36-20"
Gemma,7,8,0.285714286,27.08333333,,26.66666667,28.57142857,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path google/gemma-7b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Gemma-7B/lora/train_2024-07-07-09-36-31 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path google/gemma-7b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Gemma-7B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Gemma-7B/lora/train_2024-07-07-09-36-31"
Gemma,7,12,0.642857143,59.06432749,,79.16666667,64.28571429,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path google/gemma-7b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Gemma-7B/lora/train_2024-07-07-09-36-30 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path google/gemma-7b \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Gemma-7B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Gemma-7B/lora/train_2024-07-07-09-36-30"
Qwen,1.8,12,0.714285714,68.88888889,0.714285714,81.81818182,71.42857143,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen-1_8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen-1.8B/lora/train_2024-07-06-12-16-05 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen-1_8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen-1.8B/lora/eval_2024-07-06-12-16-05 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen-1.8B/lora/train_2024-07-06-12-16-05"
Qwen,1.8,8,0.714285714,68.88888889,0.714285714,81.81818182,71.42857143,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen-1_8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen-1.8B/lora/train_2024-07-06-12-16-05 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen-1_8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen-1.8B/lora/eval_2024-07-06-12-16-05 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen-1.8B/lora/train_2024-07-06-12-16-06"
Qwen,1.8,4,0.642857143,64.1025641,0.642857143,64.58333333,64.28571429,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen-1_8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen-1.8B/lora/train_2024-07-06-12-16-07 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen-1_8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen-1.8B/lora/eval_2024-07-06-12-16-07 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen-1.8B/lora/train_2024-07-06-12-16-06"
Qwen,7,12,0.571428571,57.14285714,,57.14285714,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen-7B/lora/QWEN-7B-16 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
--lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen-7B/lora/eval_2024-07-07-10-58-39 \
    --do_predict True \
--adapter_name_or_path saves/Qwen-7B/lora/QWEN-7B-16"
Qwen,7,8,0.571428571,56.25,,57.77777778,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen-7B/lora/QWEN-7B-8 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
--lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen-7B/lora/eval_2024-07-07-10-58-39 \
    --do_predict True \
--adapter_name_or_path saves/Qwen-7B/lora/QWEN-7B-8"
Qwen,7,4,0.5,47.59358289,,50,50,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen-7B/lora/QWEN-7B-4 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
--lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen-7B/lora/eval_2024-07-07-10-58-39 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen-7B/lora/QWEN-7B-4"
Qwen,14,12,0.571428571,53.33333333,,60.60606061,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen-14B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen-14B/lora/QWEN-14B-None \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all ","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen-14B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen-14B/lora/eval_2024-07-07-10-58-39 \
    --do_predict True \
--adapter_name_or_path saves/Qwen-14B/lora/QWEN-14B-None

{"
Qwen,14,8,0.571428571,53.33333333,,60.60606061,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen-14B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen-14B/lora/QWEN-14B-8 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
--lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen-14B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen-14B/lora/eval_2024-07-07-10-58-39 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen-14B/lora/QWEN-14B-8"
Qwen,14,4,0.571428571,47.5,,76.92307692,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen-14B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen-14B/lora/QWEN-14B-4 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
--lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen-14B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen-14B/lora/eval_2024-07-07-10-58-39 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen-14B/lora/QWEN-14B-4"
Qwen1.5,0.5,12,0.785714286,78.46153846,,79.16666667,78.57142857,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-0.5B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-0.5B/lora/train_2024-07-07-09-36-18 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-0.5B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-0.5B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-0.5B/lora/train_2024-07-07-09-36-18"
Qwen1.5,0.5,8,0.642857143,59.06432749,,79.16666667,64.28571429,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-0.5B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-0.5B/lora/train_2024-07-07-09-36-18 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-0.5B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-0.5B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-0.5B/lora/train_2024-07-07-09-36-19"
Qwen1.5,0.5,4,0.571428571,56.25,,57.77777778,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-0.5B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-0.5B/lora/train_2024-07-07-09-36-20 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --adapter_name_or_path saves/Qwen1.5-0.5B/lora/train_2024-07-07-09-36-19 \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-0.5B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-0.5B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-0.5B/lora/train_2024-07-07-09-36-20"
Qwen1.5,1.8,12,0.642857143,62.56684492,,67.5,64.28571429,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-1.8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-1.8B/lora/train_2024-07-07-09-36-35 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-1.8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-1.8B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-1.8B/lora/train_2024-07-07-09-36-35"
Qwen1.5,1.8,8,0.714285714,70.83333333,,73.33333333,71.42857143,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-1.8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-1.8B/lora/train_2024-07-07-09-36-40 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-1.8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-1.8B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-1.8B/lora/train_2024-07-07-09-36-35"
Qwen1.5,1.8,4,0.571428571,56.25,,57.77777778,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-1.8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-1.8B/lora/train_2024-07-07-09-36-41 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --adapter_name_or_path saves/Qwen1.5-1.8B/lora/train_2024-07-07-09-36-35 \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-1.8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-1.8B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-1.8B/lora/train_2024-07-07-09-36-41"
Qwen1.5,4,4,0.428571429,41.66666667,,42.22222222,2.857142857,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-4B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-4B/lora/train_2024-07-07-09-36-222 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-4B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-4B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-4B/lora/train_2024-07-07-09-36-222"
Qwen1.5,4,8,0.571428571,56.25,,57.77777778,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-4B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-4B/lora/train_2024-07-07-09-36-223 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --adapter_name_or_path saves/Qwen1.5-4B/lora/train_2024-07-07-09-36-222 \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-4B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-4B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-4B/lora/train_2024-07-07-09-36-223"
Qwen1.5,4,12,0.571428571,56.25,,57.77777778,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-4B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 20.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-4B/lora/train_2024-07-07-09-36-223 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-4B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-4B/lora/eval_2024-07-07-09-36-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-4B/lora/train_2024-07-07-09-36-224"
Qwen1.5,7,12,0.571428571,0.571428571,,0.571428571,0.571428571,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-7B/lora/train_2024-07-07-11-57-18 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-7B/lora/eval_2024-07-07-11-57-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-7B/lora/train_2024-07-07-11-57-18"
Qwen1.5,7,8,0.571428571,0.571428571,,0.571428571,0.571428571,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-7B/lora/train_2024-07-07-11-57-19 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all"," llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-7B/lora/eval_2024-07-07-11-57-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-7B/lora/train_2024-07-07-11-57-19"
Qwen1.5,7,4,0.571428571,53.33333333,,60.60606061,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/Qwen1.5-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/Qwen1.5-7B/lora/train_2024-07-07-11-57-20 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/Qwen1.5-7B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/Qwen1.5-7B/lora/eval_2024-07-07-11-57-18 \
    --do_predict True \
    --adapter_name_or_path saves/Qwen1.5-7B/lora/train_2024-07-07-11-57-20"
Llama3,8,4,0.85364356,85.364356,,85.364356,81.81818182,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/llama3-8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/llama3-8B/lora/train_2024-07-07-11-57-20 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/llama3-8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/llama3-8B/lora/eval_2024-07-07-11-57-18 \
    --do_predict True \
    --adapter_name_or_path saves/llama3-8B/lora/train_2024-07-07-11-57-20"
Llama3,8,8,0.61457745,61.457745,,42.22222222,64.28571429,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/llama3-8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/llama3-8B/lora/train_2024-07-07-11-57-20 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/llama3-8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_bit 8 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/llama3-8B/lora/eval_2024-07-07-11-57-18 \
    --do_predict True \
    --adapter_name_or_path saves/llama3-8B/lora/train_2024-07-07-11-57-20"
Llama3,8,12,0.54475457,41.66666667,,57.77777778,57.14285714,"llamafactory-cli train \
    --stage sft \
    --do_train True \
    --model_name_or_path Qwen/llama3-8B \
    --preprocessing_num_workers 16 \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_train \
    --cutoff_len 1024 \
    --learning_rate 0.001 \
    --num_train_epochs 10.0 \
    --max_samples 100000 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --max_grad_norm 1.0 \
    --logging_steps 5 \
    --save_steps 100 \
    --warmup_steps 0 \
    --optim adamw_torch \
    --packing False \
    --report_to none \
    --output_dir saves/llama3-8B/lora/train_2024-07-07-11-57-20 \
    --bf16 True \
    --plot_loss True \
    --ddp_timeout 180000000 \
    --include_num_input_tokens_seen True \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0 \
    --lora_target all","llamafactory-cli train \
    --stage sft \
    --model_name_or_path Qwen/llama3-8B \
    --preprocessing_num_workers 16 \
    --finetuning_type lora \
    --quantization_method bitsandbytes \
    --template default \
    --flash_attn auto \
    --dataset_dir data \
    --dataset Chinese_audioTotext_test \
    --cutoff_len 1024 \
    --max_samples 100000 \
    --per_device_eval_batch_size 2 \
    --predict_with_generate True \
    --max_new_tokens 512 \
    --top_p 0.5 \
    --temperature 0.95 \
    --output_dir saves/llama3-8B/lora/eval_2024-07-07-11-57-18 \
    --do_predict True \
    --adapter_name_or_path saves/llama3-8B/lora/train_2024-07-07-11-57-20"
