# -*- coding: utf-8 -*-
"""你的代码llama3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EJaqxjigGaF2SbLdCH-v2TYgKX904a7j
"""

# Commented out IPython magic to ensure Python compatibility.
# Change the current directory to /content/ (default working directory in Google Colab)
# Remove any existing folder named "LLaMA-Factory" to ensure a clean environment
# Clone the LLaMA-Factory repository from GitHub
# Navigate into the cloned "LLaMA-Factory" directory
# List the files in the current directory to verify the repository structure
# Install the "unsloth" library from a specific GitHub repository, optimized for Colab usage
# Install the xformers library (a PyTorch library for efficient transformer models), version 0.0.25, without dependencies
# Install the current package (LLaMA-Factory) with support for bitsandbytes (optimized GPU computations for LLMs)
# %cd /content/
# %rm -rf LLaMA-Factory
!git clone https://github.com/hiyouga/LLaMA-Factory.git
# %cd LLaMA-Factory
# %ls
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps xformers==0.0.25
!pip install .[bitsandbytes]

# Import PyTorch to ensure it is correctly installed
# Check if a GPU is available; if not, print a setup instruction for enabling a GPU in Colab
import torch
try:
  assert torch.cuda.is_available() is True
except AssertionError:
  print("Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6")

# Commented out IPython magic to ensure Python compatibility.
# Import the json module to manipulate the identity.json file
# Navigate to the LLaMA-Factory directory to ensure the script works on files in the correct path
# Define constants for replacing placeholders in the dataset
# Open the "identity.json" dataset file, load it into a Python dictionary, and replace placeholders with actual values
# Replace placeholders "NAME" and "AUTHOR" in the "output" field of each sample
# Save the updated dataset back to the "identity.json" file
import json

# %cd /content/LLaMA-Factory/

NAME = "Llama-3"
AUTHOR = "LLaMA Factory"

with open("data/identity.json", "r", encoding="utf-8") as f:
  dataset = json.load(f)

for sample in dataset:
  sample["output"] = sample["output"].replace("NAME", NAME).replace("AUTHOR", AUTHOR)

with open("data/identity.json", "w", encoding="utf-8") as f:
  json.dump(dataset, f, indent=2, ensure_ascii=False)

# Install required dependencies listed in the repository's requirements.txt file
# Install bitsandbytes library, which provides low-level GPU optimizations for machine learning models
# Install the rouge-chinese library, used for evaluating text summarization tasks in Chinese
# Install the Accelerate library, which helps efficiently run models on multi-GPU setups
# Install tiktoken, a fast tokenizer for LLMs such as GPT
# Install transformers_stream_generator, likely used for streaming text generation from transformer models
# Install bitsandbytes from PyPI (Python Package Index) again to ensure compatibility
# Install the accelerate library for managing hardware and distribution strategies
# Install specific versions of the transformers library to ensure compatibility
# Install the flash_attn library, which provides optimized implementations for attention mechanisms
# Install llama_index, used for constructing, managing, and querying large language model-based indices
!pip install -r requirements.txt
!pip install bitsandbytes
!pip install rouge-chinese
!pip install Accelerate
!pip install tiktoken
!pip install transformers_stream_generator
!pip install -i https://pypi.org/simple/ bitsandbytes
!pip install accelerate
!pip install transformers==4.39.3
!pip install transformers==4.40.0
!pip install transformers==4.40.1
!pip install flash_attn!pip install llama_index
!pip install llama-index llama-index.embeddings.huggingface

# Import the os module for environment variable management
# Import the userdata module from Google Colab to access user data securely
# Set the Hugging Face token (if available) as an environment variable for authenticated access
import os
from google.colab import userdata
os.environ["HF_TOKEN"] = userdata.get('HF_TOKEN')

# Commented out IPython magic to ensure Python compatibility.
# Reset the current environment, clearing all variables and imports
# %reset -f

# Commented out IPython magic to ensure Python compatibility.

# %cd /content/LLaMA-Factory/  # Return to the LLaMA-Factory directory to ensure commands run in the correct location
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git" # Reinstall unsloth and xformers libraries to ensure a clean setup
!pip install --no-deps xformers==0.0.25
!pip install .[bitsandbytes] # Reinstall the LLaMA-Factory package with bitsandbytes support
!pip install scikit-learn # Install scikit-learn, a machine learning library often used for evaluating models

# Start the LLaMA Factory web interface with GRADIO_SHARE set to 1 for public sharing
!GRADIO_SHARE=1 llamafactory-cli webui